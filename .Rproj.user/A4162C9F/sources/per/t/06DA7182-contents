---
title: "exercise1"
author: "Patrik Palencar"
output: html_document
date: "2024-03-09"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Loading packages that I will need to complete the exercise. I will use the data.table package because it is easier and more efective to work with it in comparison to data.frame. Plotly to use nice plots and caret to use Machine Learning functions.
```{r packages}
library(data.table)
library(plotly)
library(caret)
```

## Data loading and exploration

Loading the data with fread 
```{r loading the data}
training_data <- fread("airquality.training.csv")
test_data <- fread("airquality.test.csv")
```


I can see the structure and head of the data and get a good imagination of data.
```{r structure of data}
head(training_data)
head(test_data)
```
I can already see here, that there are some NA's that I will need to get rid of.



Here I am exploring number of explorations and number of the NA's in whole dataset
```{r observing of the counts of data}
print(paste("number of observations in training data:",training_data[,.N]))
print(paste("number of NA's in training data:",sum(is.na(training_data))))
print(paste("number of observations in test data:",test_data[,.N]))
print(paste("number of NA's in test data:",sum(is.na(test_data))))
```
There is total of 122 observations for each column and 34 NA's in training data set and 31 observations and 10 NA's in the test data set



Next I will compare the distribution training data vs test data in %
```{r counting distribution}
#first I remove the NA's from both data sets
train_data_nonas <- na.omit(training_data)
test_data_nonas <- na.omit(test_data)

train_number_of_observations <- training_data[,.N]
test_number_of_observations <- test_data[,.N]
total_observations <- train_number_of_observations + test_number_of_observations
distribution_percentage_train <- train_number_of_observations / total_observations * 100
distribution_percentage_test <- test_number_of_observations / total_observations * 100
distribution_percentage_test
distribution_percentage_train
```
I can see that there is distribution of test data set vs train data set, approximately 20% to 80%

## Plotting of the data

I will use plotly to display descriptive plots about the data (e.g., scatter plots Ozone vs. rest, boxplots of all variables, evolution over time, ...).
```{r scatter plots of ozone vs variables}
ozone_vs_wind <- plot_ly(data = train_data_nonas, type ="scatter", mode="markers" , x =~Wind  , y =~Ozone)
ozone_vs_temp<- plot_ly(data = train_data_nonas, type = "scatter", mode = "markers", x=~Temp, y=~Ozone)
ozone_vs_solarR <- plot_ly(data = train_data_nonas, type ="scatter", mode="markers" , x =~Solar.R  , y =~Ozone)

ozone_vs_wind
ozone_vs_temp
ozone_vs_solarR
```
On these plots we can see that there is a correlation in each comparison. Ozone vs Wind is negatively correlated. Ozone vs temp and ozone vs SolarR are both positively correlated. Interesting is that, the warmer it is the ozone levels are higher.


Next I will plot the variables based on time.
```{r scatter line plots}
#column of dates
train_data_dates <- train_data_nonas[, Date:= as.Date(paste("2024",Month ,Day, sep="-"),format= "%Y-%m-%d")]
head(train_data_dates)
train_data_per_date_ozone <- plot_ly(train_data_dates, x = ~Date, y = ~Ozone, type = "scatter", mode = "lines", name= "Ozone per date")
train_data_per_date_solarR <- plot_ly(train_data_dates, x = ~Date, y = ~Solar.R, type = "scatter", mode = "lines",name= "Solar Radiation per date")
train_data_per_date_wind <- plot_ly(train_data_dates, x = ~Date, y = ~Wind, type = "scatter", mode = "lines",name= "Wind per date")
train_data_per_date_temp <- plot_ly(train_data_dates, x = ~Date, y = ~Temp, type = "scatter", mode = "lines",name= "Temperature per date")

train_data_per_date_ozone
train_data_per_date_solarR
train_data_per_date_wind
train_data_per_date_temp
```
First I created new column of dates to be able to make nice plots.
From the plot Ozone per Date I can see that the ozone levels are highest in summer months and also that there are big jumps of ozone levels from week to week.
From the plot SolarR per Date I can see that Solar radiation levels are pretty much equal during whole measured period.
From the plot Wind per Date I can say that months may and june were more windy then the rest of the months and also that there are big jumps of wind speed from week to week.
From the plot Temperature per Date I can see that the summer months (jun, july, august) are the warmest.


Next, I show all of the variables in box plots
```{r box plots}
ozone_box <- plot_ly(train_data_nonas, type = "box", x=~Ozone,name ="Ozone Box plot")
solar_box <- plot_ly(train_data_nonas, type = "box", x=~Solar.R,name ="Solar Radiation Box plot")
wind_box <- plot_ly(train_data_nonas, type = "box", x=~Wind, name = "Wind Box Plot")
temp_box <- plot_ly(train_data_nonas, type = "box", x=~Temp, name = "Temperature Box Plot")

solar_box
ozone_box
wind_box
temp_box
```
I created a box plot for each variable and looked at them.We can have a better grasp of the data because box plots show us basic statistical data such as median, min, max or interquartile range.
From looking at the wind box plot I can say that the data has low variability because the IQR is small and also that a lot of data will be concentrated in 


## Training and evaluating the data sets 

Here I created a plot showing RMSE values of training data and test data on model with degree from 1 to 5. 
```{r  training and evaluating}
models <- list()
RMSE_training <- vector()
RMSE_test <- vector()


for (d in 1:5){
  models[[d]] <- lm(Ozone ~ poly(Solar.R, Wind, Temp ,degree = d), data = train_data_nonas)
  
  training_predictions <- predict(models[[d]], train_data_nonas)
  train_actuals <- train_data_nonas[,Ozone]
  RMSE_training[d] <- RMSE(training_predictions, train_actuals)

  testing_predictions <- predict(models[[d]], test_data_nonas)
  test_actuals <- test_data_nonas[,Ozone]
  RMSE_test[d] <- RMSE(testing_predictions, test_actuals)
}

plot_ly(x = 1:length(RMSE_training),y = RMSE_training, type = "scatter", mode = "line", name = "Train data") %>% add_lines(x = 1:length(RMSE_test), y = RMSE_test, name = "Test data") %>%layout(title = "RMSE Comparison for Training and Testing", xaxis = list(title = "Degree"), yaxis = list(title = "RMSE"))
```
#### I trained the model with test data and train data with each complexity degree. I can see that more complex the model is the more is the model ovefitting. Based on the plot and tradeoffs of bias and variance, I would choose complexity with a degree of 3 because the overfit isn't that big and the RMSE values are low (lower as with degree 2).