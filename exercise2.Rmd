---
title: "exercise2"
output: html_document
author: "Patrik Palencar"
date: "2024-04-01"
---


# Exercise 2
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loading libraries}
library(data.table)
library(plotly)
library(caret)
library(rpart)
library(rpart.plot)
library(mlr3)
library(gridExtra)
library(e1071)
library(ranger)
library(class)
library(rmarkdown)
```


```{r Loading datasets, echo=FALSE}
train_data <- fread("eeg_training.csv")
test_data <- fread("eeg_test.csv")
```
## Data Exploration and Data Preprocessing
### Data Exploration

```{r exploring data}
str(train_data)
paged_table(head(train_data))
paged_table(head(test_data))
summary(train_data)
summary(test_data)
colSums(is.na(train_data))

column_types <- sapply(train_data, class)
column_types
```
We can see the structure of the datasets and I called summary and I also check for types of data in each column so I can have better understanding of the data.
I can see that in the trainset column V14 has 10319 NA's, meaning that this column is useless.
Some columns have very big max values.
Now I will plot the data and see if I get some interesting insights

```{r plotting}
plots1 <- list()
for (i in colnames(train_data[, .(V1,V2,V3,V4,V5,V6)])) {
  plots1[[i]] <- plot_ly(data = train_data, x = 1:nrow(train_data), y = train_data[,get(i)], color =train_data[,V17],
                        type = "scatter", mode = "markers")
}
plots2 <- list()
for (i in colnames(train_data[, .(V7,V8,V9,V10,V11,V12)])) {
  plots2[[i]] <- plot_ly(data = train_data, x = 1:nrow(train_data), y = train_data[,get(i)], color =train_data[,V17],
                        type = "scatter", mode = "markers")
}
plots3 <- list()
for (i in colnames(train_data[, .(V13,V14,V15,V16)])) {
  plots3[[i]] <- plot_ly(data = train_data, x = 1:nrow(train_data), y = train_data[,get(i)], color =train_data[,V17],
                        type = "scatter", mode = "markers")
}

subplot(plots1, nrows =3 , titleY = T, margin = 0.05)
subplot(plots2, nrows =3 , titleY = T, margin = 0.05)
subplot(plots3, nrows =2 , titleY = T, margin = 0.05)

```

On these plots we can see that we have few outlayers in each variable. These outliers can possibly be errors or anomalies. In the **V4,V5,V8,V16** it is clear that the outliers must be error because its value is more than 300 000. On the others it is possible that the outliers are just anomalies. I will remove the error outliers in data cleaning.
We can also see that in **V14** there is significantly less values because there is more than 10000 na's in V14 column. I will need to do something with this column in data cleaning.

```{r exploring data2}
print(paste("number of observations in training data:",nrow(train_data)))
print(paste("number of NA's in train data:",sum(is.na(train_data))))
print(paste("number of observations in test data:",nrow(test_data)))
print(paste("number of NA's in test data:",sum(is.na(test_data))))

sum(is.na(train_data[,V14]))
train_data[, V14 := NULL]
test_data[,V14:=NULL]
print(paste("number of na's in training data after deleting V14 column:",sum(is.na(train_data))))
na.omit(train_data)
```
We can see that there are around 10500 total observations.
There is total of **10340** NA's in training data set. I already know that most of the NA's (10319) are in the V14 column so i will just delete it. After deleting the column there is only 21 NA's that I can omit.
There are no NA's in test data set

```{r cleaning and exploring data}
train_number_of_observations <- train_data[,.N]
test_number_of_observations <- test_data[,.N]
train_number_of_observations <- train_data[,.N]
test_number_of_observations <- test_data[,.N]

total_observations <- train_number_of_observations + test_number_of_observations
distribution_percentage_train <- train_number_of_observations / total_observations * 100
distribution_percentage_test <- test_number_of_observations / total_observations * 100

distribution_percentage_train
distribution_percentage_test
```
I calculated the train test split. The data are divided with 70/30 train test split.


### Data Preprocessing

```{r preprocessing data}
for (col in names(train_data[, !"V17"])) {
  train_data <- train_data[!(get(col) > 50000)]
}

colnames(train_data)[16] <- "eyes_status"   
colnames(test_data)[16] <- "eyes_status"

train_x <- train_data[,!"eyes_status"]
train_y <- train_data[,eyes_status]
test_x  <- test_data[,!"eyes_status"]
test_y  <- test_data[,eyes_status]
test_y <- factor(test_y, levels = c(1, 2), labels = c("1", "2"))
```
I removed rows with outliers that are likely to be error in measurement.
I changed the name of column with closed and opened eyes, now the data table makes more sense.
Separating independent variables and target variable form both training and test datasets. This prepares them for use in evaluating of clasification models. 
I am also setting test_y as factor with levels 1 and 2 for creating confusion matrixes later.

## Clasifier Training

### Training a classifier using Decision Trees.
#### Default Decision Tree

```{r default model, cache = TRUE}
dtree_default <- rpart::rpart(eyes_status ~ ., method = "class", data = train_data)
printcp(dtree_default)
plotcp(dtree_default)
rpart.plot(dtree_default, type = 2, extra = 101, fallen.leaves = F, main = "Classification Tree for eye status", tweak=1.2)
```

First default decision tree doesn't perform well as we can see the xerror on level 9 is 0.72304.
The decrease in the complexity parameter values with each split suggests that pruning the tree could improve its performance. In nine levels of CP I don't see any overfitting meaning there is space for imporvement of the DecTree.
Next I can grow the full tree and see if there will be overfitting. If yes then I can prune it afterwords.

#### Full Decision Tree
```{r full tree, cache = TRUE}
dtree_full <- rpart::rpart(eyes_status ~ ., method = "class", data = train_data,
                           control = rpart.control(minsplit = 1, cp = 0))
printcp(dtree_full)
plotcp(dtree_full)
rpart.plot(dtree_full, type = 2, extra = 101, fallen.leaves = F, tweak = 1.2, main = "Entire Tree for eye status")
best_value <- which.min(dtree_full$cptable[, "xerror"])
```

Here I created full decision tree. We can see that the full tree is very complex 64 levels of CP and the tree plot is unreadable. I will find the best value of xerror and then prune the tree. Best value of xerror is 0.53818 and it's on level 48. That means we need to prune the tree on level 48.

#### Pruned Decision Tree
```{r tree pruning, cache = TRUE}
best_cp_for_pruning <- dtree_full$cptable[best_value, "CP"]
dtree_pruned <- prune(dtree_full, cp = best_cp_for_pruning)
printcp(dtree_pruned)
rpart.plot(dtree_pruned, type = 2, extra = 101, fallen.leaves = F, tweak = 1.2, main = "Pruned Tree for eye status")
```

Here I created pruned decision tree on level 48. First I determined the best level for pruning and then I pruned the tree based on the level. We can see now that the final tree is simplier, prevents overfitting and performs better than the full tree.

### Predicting on Decision Tree model
```{r predicting on Decision Tree models}
pred_y_dtree_default <- predict(dtree_default, newdata = test_x, type = "class")
pred_y_dtree_full <- predict(dtree_full, newdata = test_x, type = "class")
pred_y_dtree_pruned <- predict(dtree_pruned, newdata = test_x, type = "class")

default_decision_tree_confusion_m <- confusionMatrix(pred_y_dtree_default, reference = test_y, positive = "2", mode = "prec_recall")
full_decision_tree_confusion_m <- confusionMatrix(pred_y_dtree_full,test_y, positive = "2", mode = "prec_recall")
pruned_decision_tree_confusion_m <- confusionMatrix(pred_y_dtree_pruned, test_y, positive = "2", mode = "prec_recall")

default_decision_tree_confusion_m
full_decision_tree_confusion_m
pruned_decision_tree_confusion_m
```
I am predicting on all 3 decision tree classifiers I created with positive class "2" = closed eyelids.
Based on confusion matrixes and calculations of Accuracy, Precision, Recall, and F1-Score we can see that the **pruned decision tree** classification model perfoms the best out of these three models.
The accuracy is 80.62% what is not best but it can be satisfactory in some cases.The same goes with Precision, Recall and F1 value.
I saved the best confusion matrix as a variable so I can create final table

### Training a classifier using SVM.
```{r linear kernel}
svm_model<- svm(eyes_status ~ ., 
                data = train_data, 
                type = "C-classification", 
                kernel = "linear",
                cost = 1,
                scale = FALSE)
svm_model
svm_model$rho
```

I created Linear SVM classification model based on train data.
We can see that it has really big amount of support vectors:5843
Then we can see the indexes of the support vectors, SV coordinates and negative eyes_status intercept of the decision boundary.
 
```{r linear kernel accuracy, cache = TRUE}
pred_train <- predict(svm_model, train_data)
mean(pred_train == train_data$eyes_status)

pred_test <- predict(svm_model, test_data)
linear_svm_matrix <- confusionMatrix(data = pred_test, reference = test_y)
linear_svm_matrix
```

We can see that the accuracy of this model is for the train set 62.46774% and for the test set 63.88518% what is bad and I wouldn't use this model for predicting.

```{r polynomial kernel, cache = TRUE}
svm_model<- svm(eyes_status ~ ., 
                data = train_data, 
                type = "C-classification", 
                kernel = "polynomial",
                cost = 1,
                degree = 2,
                scale = FALSE)
svm_model
```

Here I train the model with polynomial kernel to see if there is difference in accuracy.
```{r polynomial kernel accuracy}
pred_train <- predict(svm_model, train_data)
mean(pred_train == train_data$eyes_status)

pred_test <- predict(svm_model, test_data)
poly_svm_matrix <- confusionMatrix(data = pred_test, reference = test_y)
poly_svm_matrix
```

As the accuracy shows the polynomial kernel is little bit better than linear but it is still performing very bad.

```{r radial kernel model, cache = TRUE}
svm_model<- svm(eyes_status ~ ., 
                data = train_data, 
                type = "C-classification", 
                kernel = "radial",
                cost = 1,
                degree = 2,
                scale = FALSE)
svm_model

```


```{r radial kernel accuracy}
pred_test <- predict(svm_model, test_data)
radial_svm_matrix <- confusionMatrix(data = pred_test, reference = test_y)
radial_svm_matrix
```
We can see that I have got 100% accuracy for train data but that is because we trained the radial model on train_data.
If we look on test_data accuracy we see that this model performs horribly.

### Training a classifier using KNN.

```{r KNN classifier, cache = TRUE}
training_pred <- list()
Kselection <- seq(1, 85, 2)

for (i in Kselection) {
  training_pred[[as.character(i)]] <- knn.cv(train = train_x,
                                             cl    = train_y,
                                             k     = i)
}
get_accuracy <- function(prediction, reference) {
  all_levels <- union(levels(prediction), levels(reference))
  prediction_factor <- factor(prediction, levels = all_levels)
  reference_factor <- factor(reference, levels = all_levels)
  confusion_matrix <- confusionMatrix(data = prediction_factor, reference = reference_factor)
  accuracy <- confusion_matrix$overall["Accuracy"]
  return(accuracy)
}
accuracies <- sapply(training_pred, get_accuracy, reference = train_y)
plot_ly(x = Kselection, y = accuracies, type = "scatter", mode = "line")

test_pred <- knn(train = train_x,
                 cl    = train_y,
                 test  = test_x,
                 k     = 7)
knn_matrix <- confusionMatrix(data = test_pred, reference = test_y)
knn_matrix
```

For the knn I am finding the best number of neighbours for the algorithm and ploting the accuracies.
As I found out 7 and 9 neighbours gives the best accuracy so I trained the model with 7 neighbours for best accuracy of 91.14%

### Training a classifier using Random Forest.
```{r Random Forest classification model, cache = TRUE}
train_y <- as.factor(train_y)
rf.orig <- ranger(x = train_x, y = train_y)

confusionMatrix(data = rf.orig$predictions, reference = train_y,
                                 positive = "2", mode = "prec_recall")
my_pred <- predict(object = rf.orig, data = test_x)
rf_matrix <- confusionMatrix(data = my_pred$predictions, reference = test_y, positive = "2", mode = "prec_recall")
rf_matrix
```
We can see good accuracy precision recall and f1 score on both train and test set.
The model performs better on the test set and with accuracy 91.77% is the best model so far.
We could also rebalance the data. 

### Training a classifier using AdaBoost.
```{r adaboost, cache = TRUE}
train_data$eyes_status <- as.factor(train_data$eyes_status)

ada_list<-c()
mfinal_values <- c(10, 50, 100, 200, 300, 400)
for (mfinal in mfinal_values) {
  model <- adabag::boosting(eyes_status ~ ., data = train_data, boos = TRUE, mfinal = mfinal)
  ada_list[[as.character(mfinal)]] <- model
}

accuracy_data <- data.frame(MFinal = numeric(), Accuracy = numeric())

for (i in seq_along(ada_list)) {
  mfinal <- mfinal_values[i]
  model <- ada_list[[as.character(mfinal)]]
  predictions <- predict(model, newdata = test_data)
  accuracy <- mean(predictions$class == test_data$eyes_status)
  accuracy_data <- rbind(accuracy_data, data.frame(MFinal = mfinal, Accuracy = accuracy))
}

plot <- plot_ly(accuracy_data, type = "scatter", mode = "lines+markers", x = ~MFinal, y = ~Accuracy, name = "Accuracy")
plot

model <- ada_list[["300"]]
my_pred_adaboost <- predict(model, newdata = test_x)
adaboost_matrix <- confusionMatrix(as.factor(my_pred_adaboost$class), test_y, mode = "prec_recall", positive = "2")
adaboost_matrix
```

I trained adaboost classifier with different mfinal values (by tuning the mfinal parameter) and I found that mfinal = 300 had the best performence out of these I tried.

### Training a classifier using XGBoost.
```{r xgboost, cache=TRUE, results=FALSE}
train_x_matrix <- as.matrix(train_x)
train_y_binary <- ifelse(train_y == 1,  0, 1)
xgb <- xgboost::xgboost(data = train_x_matrix,
                        label = train_y_binary,
                        nrounds=3000, objective = "binary:logistic")
xgbtest_x <- as.matrix(test_x)
mypred <- predict(xgb,newdata = xgbtest_x)
xgbtest_y <- ifelse(test_y == 1, 0, 1)
my_pred_xgboost <- as.integer(mypred>0.5)

xgb_matrix <- confusionMatrix(as.factor(my_pred_xgboost),as.factor(xgbtest_y), mode="prec_recall", positive = "1")
xgb_matrix
```
For the xg boost model I needed to change the test_x and train_x data frames to matrix and the test_y and train_y to binary so the algorithm works.
I ran the model with different amount of rounds and 3000 seemed to me that works the best.
It has got really good accuracy of 91.34%
 
## Sumarizing table of all models
```{r summary table}
method_algos <- c("Default_tree", "Full_tree", "Pruned_tree", "Linear_svm", "Polynomial_svm", "Radial_svm", "KNN", "Random_Forest", "Adaboost", "XGboost")

accuracy_of_all <- c(default_decision_tree_confusion_m$overall["Accuracy"], full_decision_tree_confusion_m$overall["Accuracy"], pruned_decision_tree_confusion_m$overall["Accuracy"], linear_svm_matrix$overall["Accuracy"], poly_svm_matrix$overall["Accuracy"], radial_svm_matrix$overall["Accuracy"], knn_matrix$overall["Accuracy"], rf_matrix$overall["Accuracy"], adaboost_matrix$overall["Accuracy"], xgb_matrix$overall["Accuracy"])

precision_of_all <- c(default_decision_tree_confusion_m$byClass["Precision"], full_decision_tree_confusion_m$byClass["Precision"], pruned_decision_tree_confusion_m$byClass["Precision"], linear_svm_matrix$byClass["Precision"], poly_svm_matrix$byClass["Precision"], radial_svm_matrix$byClass["Precision"], knn_matrix$byClass["Precision"], rf_matrix$byClass["Precision"], adaboost_matrix$byClass["Precision"], xgb_matrix$byClass["Precision"])

recall_of_all <- c(default_decision_tree_confusion_m$byClass["Recall"], full_decision_tree_confusion_m$byClass["Recall"], pruned_decision_tree_confusion_m$byClass["Recall"], linear_svm_matrix$byClass["Recall"], poly_svm_matrix$byClass["Recall"], radial_svm_matrix$byClass["Recall"], knn_matrix$byClass["Recall"], rf_matrix$byClass["Recall"], adaboost_matrix$byClass["Recall"], xgb_matrix$byClass["Recall"])

f1_of_all <- c(default_decision_tree_confusion_m$byClass["F1"], full_decision_tree_confusion_m$byClass["F1"], pruned_decision_tree_confusion_m$byClass["F1"], linear_svm_matrix$byClass["F1"], poly_svm_matrix$byClass["F1"], radial_svm_matrix$byClass["F1"], knn_matrix$byClass["F1"], rf_matrix$byClass["F1"], adaboost_matrix$byClass["F1"], xgb_matrix$byClass["F1"])
  
summary_df <- data.frame(Method = method_algos,Accuracy = accuracy_of_all, Precision = precision_of_all, Recall = recall_of_all, F1_score = f1_of_all)

paged_df <- paged_table(summary_df)
paged_df

plot_ly(summary_df, type = "scatter", mode = "lines+markers", y = ~Accuracy, x = seq(1, nrow(summary_df)), name = "Accuracy") |>
  add_trace(y = ~Precision, x = seq(1, nrow(summary_df)), name = "Precision") |>
  add_trace(y = ~Recall, x = seq(1, nrow(summary_df)), name = "Recall") |>
  add_trace(y = ~F1_score, x = seq(1, nrow(summary_df)), name = "F1_score")|>
  layout(xaxis = list(tickmode = "array", tickvals = seq(1, nrow(summary_df)), ticktext = summary_df$Method),
  title = "Summary Plot of precisions for all approaches", yaxis = list(title = 'Values'))

```

I created a plot comparing all the approaches that I trained my data for.
We can see that the best performing model is knn with all the measures above 90 percent.
The worst performing approaches for this type of classification are all 3 types of support vector machines.